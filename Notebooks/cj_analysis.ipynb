{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca445310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blocksci\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "parser_data_directory = Path(\"/mnt/anal/config.json\")\n",
    "cluster_directory = Path(\"/mnt/anal/cluster/\")\n",
    "dumplings_directory = Path(\"/mnt/dumplings/\")\n",
    "\n",
    "chain = blocksci.Blockchain(str(parser_data_directory))\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def get_block_height_for_date(date: str) -> int:\n",
    "    return chain.range(date)[0].height\n",
    "\n",
    "def get_block_height_range(start: str, end: str) -> Tuple[int, int]:\n",
    "    return get_block_height_for_date(start), get_block_height_for_date(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "wasabi2_events_file = dumplings_directory / \"wasabi2_events.json\"\n",
    "wasabi2_txs_file = dumplings_directory / \"wasabi2_txs.json\"\n",
    "wasabi_events_file = dumplings_directory / \"wasabi_events.json\"\n",
    "wasabi_txs_file = dumplings_directory / \"wasabi_txs.json\"\n",
    "whirlpool_events_file = dumplings_directory / \"whirlpool_events.json\"\n",
    "whirlpool_txs_file = dumplings_directory / \"whirlpool_txs.json\"\n",
    "\n",
    "with open(wasabi2_events_file) as f:\n",
    "    wasabi2_events = json.load(f)\n",
    "\n",
    "# with open(wasabi2_txs_file) as f:\n",
    "#     wasabi2_txs = json.load(f)\n",
    "\n",
    "with open(wasabi_events_file) as f:\n",
    "    wasabi_events = json.load(f)\n",
    "\n",
    "# with open(wasabi_txs_file) as f:\n",
    "#     wasabi_txs = json.load(f)\n",
    "\n",
    "with open(whirlpool_events_file) as f:\n",
    "    whirlpool_events = json.load(f)\n",
    "\n",
    "# with open(whirlpool_txs_file) as f:\n",
    "#     whirlpool_txs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for txid in wasabi2_events.keys():\n",
    "    try:\n",
    "        tx = chain.tx_with_hash(txid)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    v = tx.is_ww2_coinjoin()\n",
    "    if not v:\n",
    "#         print(tx)\n",
    "        pass\n",
    "    else:\n",
    "        count += 1\n",
    "    inp: blocksci.Input\n",
    "        \n",
    "# print(count, count - len(wasabi2_events))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time filtered_ww2_coinjoins = chain.filter_ww2_coinjoin_txes(0, len(chain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1089b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_coinjoins_found = set(map(lambda x: str(x.hash), filtered_ww2_coinjoins))\n",
    "res = {'only_in_dumplings': [], 'only_in_blocksci': [], 'in_both': []}\n",
    "\n",
    "for tx in filtered_ww2_coinjoins:\n",
    "    if str(tx.hash) in ww2_coinjoins_found and str(tx.hash) in wasabi2_events:\n",
    "        res['in_both'].append(tx)\n",
    "    elif str(tx.hash) in ww2_coinjoins_found:\n",
    "        res['only_in_blocksci'].append(tx)\n",
    "        \n",
    "for tx in wasabi2_events.keys():\n",
    "    if tx not in ww2_coinjoins_found:\n",
    "        res['only_in_dumplings'].append(tx)\n",
    "        \n",
    "print(f\"Only in dumplings: {len(res['only_in_dumplings'])}, only in blocksci: {len(res['only_in_blocksci'])}, in both: {len(res['in_both'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210d4e4",
   "metadata": {},
   "source": [
    "# Basic analysis\n",
    "Here are some basic analyses for the coinjoins just to make sure nothign fishy is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(tx: blocksci.Tx) -> Tuple[int, int]:\n",
    "    return (tx.input_count, tx.output_count)\n",
    "\n",
    "def find_number_of_unique_txs(tx: blocksci.Tx) -> Tuple[int, int]:\n",
    "    return len(set([x.value for x in tx.inputs])), len(set([x.value for x in tx.outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = chain.map_spliterator(map_func=find_number_of_unique_txs, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e295e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "\n",
    "flattened_unique_inputs = [x[0] for y in unique_counts for x in y]\n",
    "flattened_unique_outputs = [x[1] for y in unique_counts for x in y]\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(f\"min: {min(flattened_unique_inputs)}, max: {max(flattened_unique_inputs)}, median: {median(flattened_unique_inputs)}\")\n",
    "\n",
    "print(\"outputs:\")\n",
    "print(f\"min: {min(flattened_unique_outputs)}, max: {max(flattened_unique_outputs)}, median: {median(flattened_unique_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxes = chain.map_spliterator(map_func=find_min_max, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, out = minmaxes[0][0]\n",
    "\n",
    "for x in minmaxes:\n",
    "    for input1, output1 in x:\n",
    "        if input1 > inp:\n",
    "            inp = input1\n",
    "        if output1 > out:\n",
    "            out = output1\n",
    "            \n",
    "print(inp, out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42483ad5",
   "metadata": {},
   "source": [
    "# Remix analysis\n",
    "\n",
    "Here we have the map functions for different remix analyses. The functions should have the following interface\n",
    "`map_func(tx: blocksci.Tx, **kwargs) -> T`\n",
    "where `T` is the common result type. The result will be added to the list of results of each worker.\n",
    "\n",
    "`kwargs` are the arguments passed to each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_remixes_within_one_hop(tx: blocksci.Tx, **kwargs):\n",
    "    \"\"\"Pass in `events` as kwarg. Will check whether there is an output of `tx` in `events`.\n",
    "    We can count this as 'remix' transaction.\n",
    "    \"\"\"\n",
    "    cj_events = kwargs['events']\n",
    "    result = (tx.hash, tx.output_count, [])\n",
    "    for c, i in enumerate(tx.outputs):\n",
    "        if not i.is_spent:\n",
    "            continue\n",
    "        \n",
    "        if str(i.spending_tx.hash) in cj_events:\n",
    "            result[2].append((i.index, True))\n",
    "            \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb68fa",
   "metadata": {},
   "source": [
    "### Results processing\n",
    "\n",
    "We take the outputs of the above functions and compute various statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b80083",
   "metadata": {},
   "outputs": [],
   "source": [
    "wasabi2_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi2_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi2_events.keys())\n",
    "wasabi_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi_events.keys())\n",
    "whirlpool_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(whirlpool_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=128, events=whirlpool_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835379c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=64, events=whirlpool_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f91015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_results = chain.map_spliterator(find_remixes_within_one_hop, list(wasabi2_events.keys()), str(parser_data_directory), workers=64, events=wasabi2_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_remix_stats(results, events, key):\n",
    "    stats_computed = {\"remix\": 0, \"left\": 0}\n",
    "    \n",
    "    for one in results:\n",
    "        for txid, all_outputs, actual in one:\n",
    "            stats_computed[\"remix\"] += len(actual)\n",
    "            stats_computed[\"left\"] += all_outputs - len(actual)\n",
    "    \n",
    "    print(key)\n",
    "    print(stats_computed)\n",
    "\n",
    "    dumplings_stats = {\"dumplings_remix\": 0}\n",
    "\n",
    "    for tx in events.values():\n",
    "        for out in tx[\"outputs\"].values():\n",
    "            if out[\"mix_event_type\"] == \"MIX_REMIX\":\n",
    "                dumplings_stats[\"dumplings_remix\"] += 1\n",
    "\n",
    "    print(dumplings_stats)\n",
    "    print(f\"dumplings - computed: {dumplings_stats['dumplings_remix'] - stats_computed['remix']}\\n\")\n",
    "\n",
    "compute_remix_stats(wasabi2_results, wasabi2_txs[\"coinjoins\"], \"wasabi2 1 hop\")\n",
    "compute_remix_stats(wasabi_results, wasabi_txs[\"coinjoins\"], \"wasabi 1 hop\")\n",
    "compute_remix_stats(whirlpool_results, whirlpool_txs[\"coinjoins\"], \"whirlpool 1 hop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02838a96",
   "metadata": {},
   "source": [
    "# Consolidation analysis\n",
    "\n",
    "The functions for consolidation analysis are here. The interface is still the same:\n",
    "`map_func(tx: blocksci.Tx, **kwargs) -> list[T]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf071fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def find_outputs_one_hop(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        output_spent_in = str(output.spending_tx.hash)\n",
    "        if output_spent_in not in found:\n",
    "            found[output_spent_in] = 0\n",
    "        found[output_spent_in] += 1\n",
    "\n",
    "\n",
    "\n",
    "def find_consolidation(tx: blocksci.Tx, **kwargs) -> List[Tuple[str, Dict[str, List[str]]]]:\n",
    "    found_for_tx = {}\n",
    "    find_outputs_one_hop(tx, found_for_tx)\n",
    "    return str(tx.hash), found_for_tx\n",
    "\n",
    "\n",
    "def has_tx_output_in_coinjoin_events(tx: blocksci.Tx, **kwargs) -> bool:\n",
    "    cj_events = kwargs[\"events\"]\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        if str(output.spending_tx.hash) in cj_events:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_outputs_two_hops(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        if output.spending_tx.output_count < 2:\n",
    "            output_spent_in = str(output.spending_tx.hash)\n",
    "            if output_spent_in not in found:\n",
    "                found[output_spent_in] = 0\n",
    "            found[output_spent_in] += 1\n",
    "            continue\n",
    "\n",
    "        for output2 in output.spending_tx.outputs:\n",
    "            if not output2.is_spent:\n",
    "                continue\n",
    "                \n",
    "            if output2.spending_tx.output_count < 2:\n",
    "                output_spent_in = str(output2.spending_tx.hash)\n",
    "                if output_spent_in not in found:\n",
    "                    found[output_spent_in] = 0\n",
    "                found[output_spent_in] += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "def find_outputs_three_hops(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "            \n",
    "        found_match = False\n",
    "\n",
    "        if output.spending_tx.output_count < 2:\n",
    "            output_spent_in = str(output.spending_tx.hash)\n",
    "            if output_spent_in not in found:\n",
    "                found[output_spent_in] = 0\n",
    "            found[output_spent_in] += 1\n",
    "            continue\n",
    "\n",
    "        for output2 in output.spending_tx.outputs:\n",
    "            if found_match:\n",
    "                break\n",
    "            if not output2.is_spent:\n",
    "                continue\n",
    "                \n",
    "            if output2.spending_tx.output_count < 2:\n",
    "                output_spent_in = str(output2.spending_tx.hash)\n",
    "                if output_spent_in not in found:\n",
    "                    found[output_spent_in] = 0\n",
    "                found[output_spent_in] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "            \n",
    "            for output3 in output2.spending_tx.outputs:\n",
    "                if not output3.is_spent:\n",
    "                    continue\n",
    "\n",
    "                if output3.spending_tx.output_count < 2:\n",
    "                    output_spent_in = str(output3.spending_tx.hash)\n",
    "                    if output_spent_in not in found:\n",
    "                        found[output_spent_in] = 0\n",
    "                    found[output_spent_in] += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "                    \n",
    "def find_consolidation_three_hops(tx: blocksci.Tx):\n",
    "    found_for_tx = {}\n",
    "    find_outputs_three_hops(tx, found_for_tx)\n",
    "    return (str(tx.hash), found_for_tx)\n",
    "\n",
    "\n",
    "def find_consolidation_two_hops(tx: blocksci.Tx):\n",
    "    found_for_tx = {}\n",
    "    find_outputs_two_hops(tx, found_for_tx)\n",
    "    return (str(tx.hash), found_for_tx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)\n",
    "# %time wasabi_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation = chain.map_spliterator(map_func=find_consolidation, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation = chain.map_spliterator(map_func=find_consolidation, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cb1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_consolidation = chain.map_spliterator(map_func=find_consolidation, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095695bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ed9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea7b7b",
   "metadata": {},
   "source": [
    "### Consolidation analyses\n",
    "\n",
    "Following are the functions computing the actual result analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57a7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_consolidated_txes_in_one_hop(consolidated):\n",
    "    total_outgoing = 0\n",
    "    counts_of_consolidated_txes_in_one_hop = defaultdict(int)\n",
    "    for root_tx, dct in consolidated:\n",
    "            for outbound_tx, count in dct.items():\n",
    "                if outbound_tx in wasabi2_events or outbound_tx in wasabi_events or outbound_tx in whirlpool_events:\n",
    "                    continue\n",
    "\n",
    "                counts_of_consolidated_txes_in_one_hop[count] += 1\n",
    "    return counts_of_consolidated_txes_in_one_hop\n",
    "\n",
    "\n",
    "# w2_consolidation_1hop = compute_consolidated_txes_in_one_hop(wasabi2_consolidation)\n",
    "# w_consolidation_1hop = compute_consolidated_txes_in_one_hop(wasabi_consolidation)\n",
    "# whirl_consolidation_1hop = compute_consolidated_txes_in_one_hop(whirlpool_consolidation)\n",
    "\n",
    "def plot_barplot_from_dictionary(dct):\n",
    "    plt.bar(range(len(dct)), list(dct.values()), align='center')\n",
    "    plt.xticks(range(len(dct)), list(dct.keys()))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def make_graph(data, name):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(8, 9))\n",
    "\n",
    "    # Plot the data for 2-8 on the first subplot\n",
    "    ax1.bar(list(range(2, 9)), [data[key] for key in range(2, 9)])\n",
    "    ax1.set_xlabel('X-axis')\n",
    "    ax1.set_ylabel('Y-axis (2-8)')\n",
    "    ax1.set_title(f'{name}: Bar Plot (2-8)')\n",
    "\n",
    "    # Plot the data for 9-15 on the second subplot\n",
    "    ax2.bar(list(range(9, 16)), [data[key] for key in range(9, 16)])\n",
    "    ax2.set_xlabel('X-axis')\n",
    "    ax2.set_ylabel('Y-axis (9-15)')\n",
    "    ax2.set_title(f'{name}: Bar Plot (9-15)')\n",
    "\n",
    "    ax3.bar(list(range(16, 23)), [data[key] for key in range(16, 23)])\n",
    "    ax3.set_xlabel('X-axis')\n",
    "    ax3.set_ylabel('Y-axis (16-22)')\n",
    "    ax3.set_title(f'{name}: Bar Plot (16-22)')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# print(f\"w2 unpaired: {w2_consolidation_1hop[1]}\")\n",
    "# print(f\"w unpaired: {w_consolidation_1hop[1]}\")\n",
    "# print(f\"whirlpool unpaired: {whirl_consolidation_1hop[1]}\")\n",
    "\n",
    "# make_graph(w2_consolidation_1hop, \"wasabi2\")\n",
    "# make_graph(w_consolidation_1hop, \"wasabi\")\n",
    "# make_graph(whirl_consolidation_1hop, \"whirlpool\")\n",
    "\n",
    "\n",
    "\n",
    "# w2_2hop_consolidated = compute_consolidated_txes_in_one_hop(wasabi2_consolidation_two_hop)\n",
    "# w_2hop_consolidated = compute_consolidated_txes_in_one_hop(wasabi_consolidation_two_hop)\n",
    "# wh_2hop_consolidated = compute_consolidated_txes_in_one_hop(whirlpool_consolidation_two_hop)\n",
    "\n",
    "\n",
    "# make_graph(w2_2hop_consolidated, \"two hop consolidation to one output wasabi2\")\n",
    "# make_graph(w_2hop_consolidated, \"two hop consolidation to one output wasabi\")\n",
    "# make_graph(wh_2hop_consolidated, \"two hop consolidation to one output whirlpool\")\n",
    "\n",
    "print(len(res_w2))\n",
    "w2_consolidation_3hop = compute_consolidated_txes_in_one_hop(res_w2)\n",
    "w_consolidation_3hop = compute_consolidated_txes_in_one_hop(res_w1)\n",
    "\n",
    "\n",
    "print(f\"w2 unpaired: {w2_consolidation_3hop[1]}\")\n",
    "print(f\"w unpaired: {w_consolidation_3hop[1]}\")\n",
    "\n",
    "\n",
    "make_graph(w2_consolidation_3hop, \"wasabi2\")\n",
    "make_graph(w_consolidation_3hop, \"wasabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e2939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'\n",
    "consolidated_tx = '0abebd6704fcd886b1e74815ce05a24a11aa2d0e543729d6dbd18629c72874a7'\n",
    "\n",
    "in_tx = chain.tx_with_hash(in_tx)\n",
    "consolidated_tx = chain.tx_with_hash(consolidated_tx) \n",
    "\n",
    "print(consolidated_tx.output_count)\n",
    "\n",
    "\n",
    "from typing import Optional, Set\n",
    "\n",
    "consolidated_txs_lost = list(find_consolidation_two_hops(in_tx))[1]\n",
    "\n",
    "# print(consolidated_txs)\n",
    "consolidated_txs = [x for x, y in consolidated_txs_lost.items() if y < 10]\n",
    "print(len(consolidated_txs), consolidated_txs_lost[consolidated_txs[0]])\n",
    "\n",
    "def subset_sum_rec(nums: List[int], total: int, start: int, memo: Dict[Tuple[int, int], Optional[Set[int]]]) -> Optional[Set[int]]:\n",
    "    if total == 0:\n",
    "        return set()\n",
    "\n",
    "    if start == len(nums):\n",
    "        return None\n",
    "\n",
    "    key = (start, total)\n",
    "    if key in memo:\n",
    "        return memo[key]\n",
    "\n",
    "    num = nums[start]\n",
    "    if num.value > total:\n",
    "        memo[key] = None\n",
    "        return None\n",
    "\n",
    "    result = subset_sum_rec(nums, total - num.value, start + 1, memo)\n",
    "    if result is not None:\n",
    "        result.add(num)\n",
    "        memo[key] = result\n",
    "        return result\n",
    "\n",
    "    result = subset_sum_rec(nums, total, start + 1, memo)\n",
    "    memo[key] = result\n",
    "    return result\n",
    "\n",
    "def find_sum_candidates(tx, inputs, output_value):\n",
    "    memo = {}\n",
    "    sorted_inputs = sorted(filter(lambda y: y.value <= output_value, inputs), key=lambda x: x.value)\n",
    "    return subset_sum_rec(sorted_inputs, output_value, 0, memo)\n",
    "\n",
    "a = set()\n",
    "for i in range(3):\n",
    "    inputs = set(in_tx.inputs) - a\n",
    "    value = chain.tx_with_hash(consolidated_txs[i]).outputs[0].value\n",
    "    r = find_sum_candidates(in_tx, inputs, value)\n",
    "    if r:\n",
    "        r = a\n",
    "\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'\n",
    "consolidated_tx = '0abebd6704fcd886b1e74815ce05a24a11aa2d0e543729d6dbd18629c72874a7'\n",
    "\n",
    "in_tx = chain.tx_with_hash(in_tx)\n",
    "consolidated_tx = chain.tx_with_hash(consolidated_tx) \n",
    "\n",
    "print(in_tx)\n",
    "print(consolidated_tx.inputs[0])\n",
    "print(consolidated_tx.inputs[0].spent_output.address_type)\n",
    "\n",
    "print(in_tx.block.timestamp)\n",
    "print(in_tx.outputs[0].block.timestamp)\n",
    "print(in_tx.outputs[0].spending_tx.block.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fecef9",
   "metadata": {},
   "source": [
    "# embrace VUT\n",
    "\n",
    "Let's try to use some analyses with [coinomon](https://coinomon.bazar.nesad.fit.vutbr.cz/#/Authentication/login).\n",
    "\n",
    "- get all coinjoins in one month (say Feb 23)\n",
    "- get all output txs from them (with one output)\n",
    "- pick one randomly and get some data about them from coinomon\n",
    "- ???\n",
    "- profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d258f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CoinomonClient:\n",
    "    def __init__(self, token: str) -> None:\n",
    "        self.crypto = \"BTC\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        self.base_url = \"https://coinomon.bazar.nesad.fit.vutbr.cz/\"\n",
    "    \n",
    "    def get_address_info(self, address: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"{self.base_url}jwt/v1/{self.crypto}/cryptoaddress/{address}/summary\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response.text))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    \n",
    "    def get_cluster_info(self, cluster_id: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"/jwt/v1/{self.crypto}/cryptocluster/{cluster_id}\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    def get_cluster_addresses(self, cluster_id: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"/jwt/v1/{self.crypto}/cryptocluster/{cluster_id}/addresses\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "token = \"insert here\"\n",
    "\n",
    "coinomon_client = CoinomonClient(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(chain.filter_in_keys)\n",
    "%time txes = chain.filter_in_keys(wasabi2_events, 0, len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568926c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e622f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start, end = get_block_height_range('2023-02-01', '2023-02-28')\n",
    "# %time res = chain.find_consolidation_3_hops(wasabi2_events, start, end)\n",
    "%time res_w1 = chain.find_consolidation_3_hops(wasabi_events, 0, len(chain))\n",
    "# %time res_w2 = chain.find_consolidation_3_hops(wasabi2_events, 0, len(chain))\n",
    "# %time res_wh = chain.find_consolidation_3_hops(whirlpool_events, 0, len(chain))\n",
    "\n",
    "# %time res = chain.find_consolidation_3_hops(whirlpool_events, 774513, 778584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea94e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "max_outputs = \"\", \"\", 0\n",
    "            \n",
    "for tx, outputs in res:\n",
    "    for out, val in outputs.items():\n",
    "        if val > max_outputs[2]:\n",
    "            max_outputs = out, tx, val\n",
    "\n",
    "print(len(res))\n",
    "print(max_outputs)\n",
    "tx = chain.tx_with_hash(max_outputs[0])\n",
    "coinomon_data = coinomon_client.get_address_info(tx.outputs[0].address.address_string)\n",
    "coinomon_data[\"data\"].pop(\"firstTx\")\n",
    "coinomon_data[\"data\"].pop(\"lastTx\")\n",
    "print(json.dumps(coinomon_data, indent=4))\n",
    "\n",
    "tx_start = chain.tx_with_hash(max_outputs[1])\n",
    "\n",
    "coinomon_data = coinomon_client.get_address_info(tx_start.outputs[0].address.address_string)\n",
    "coinomon_data[\"data\"].pop(\"firstTx\")\n",
    "coinomon_data[\"data\"].pop(\"lastTx\")\n",
    "print(json.dumps(coinomon_data, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aaf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx, _ in res:\n",
    "    tx = chain.tx_with_hash(tx)\n",
    "    coinomon_data = coinomon_client.get_address_info(tx.outputs[0].address.address_string)\n",
    "    if coinomon_data[\"data\"][\"alarms\"]:\n",
    "        print(coinomon_data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56643b19",
   "metadata": {},
   "source": [
    "# Juralysis\n",
    "\n",
    "Try some BlockSci clustering on some coinjoin output.\n",
    "Let's pick the same way as before - Feb2023, one with largest tx count (why not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac13736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time jura_res = chain.find_consolidation_3_hops(wasabi_events, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d931e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_heuristic = blocksci.heuristics.change.legacy\n",
    "\n",
    "cm = blocksci.cluster.ClusterManager.create_clustering(\"/mnt/anal/cluster\", chain, heuristic=legacy_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "jura_tx = chain.tx_with_hash(list(jura_res[0][1].keys())[0]) # don't ask\n",
    "cluster = cm.cluster_with_address(jura_tx.outputs[0].address)\n",
    "print(cluster.address_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in cm.clusters().to_list():\n",
    "    cnt += 1\n",
    "    \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = chain.tx_with_hash(res_w2[-3][0]).outputs\n",
    "m_o = map(lambda x: x.value, outputs)\n",
    "print(list(sorted(m_o, reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "outputs = {}\n",
    "\n",
    "for k in wasabi2_events:\n",
    "    try:\n",
    "        tx = chain.tx_with_hash(k)\n",
    "    except Exception:\n",
    "#         print(k, \"not found\")\n",
    "        pass\n",
    "    for out in tx.outputs:\n",
    "        value = out.value\n",
    "        if value not in outputs:\n",
    "            outputs[value] = 0\n",
    "        outputs[value] += 1\n",
    "    \n",
    "# import json\n",
    "# print(json.dumps(outputs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eaa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = list(sorted(outputs.items(), key=lambda x: -x[1]))\n",
    "a = {k: v for k, v in f}\n",
    "# print(json.dumps(a, indent=4))\n",
    "print(len(list(filter(lambda x: x[1] > 10, a.items()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e533de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [5000, 6561, 8192, 10000, 13122, 16384, 19683, 20000, 32768, 39366, 50000, 59049, 65536, 100000, 118098, 131072,\n",
    " 177147, 200000, 262144, 354294, 500000, 524288, 531441, 1000000, 1048576, 1062882, 1594323, 2000000, 2097152,\n",
    "3188646, 4194304, 4782969, 5000000, 8388608, 9565938, 10000000, 14348907, 16777216, 20000000, 28697814, 33554432,\n",
    "43046721, 50000000, 67108864, 86093442, 100000000, 129140163, 134217728, 200000000, 258280326, 268435456, 387420489,\n",
    "500000000, 536870912, 774840978, 1000000000, 1073741824, 1162261467, 2000000000, 2147483648, 2324522934, 3486784401,\n",
    "4294967296, 5000000000, 6973568802, 8589934592, 10000000000, 10460353203, 17179869184, 20000000000, 20920706406,\n",
    "31381059609, 34359738368, 50000000000, 62762119218, 68719476736, 94143178827, 100000000000, 137438953472]\n",
    "\n",
    "\n",
    "for i in common:\n",
    "    print(f\"{i}: {a.get(i)}\")\n",
    "# print(a[10000000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90d58e",
   "metadata": {},
   "source": [
    "# Analysis over the coinjoins (friends do not pay)\n",
    "**(petrs request)**\n",
    "\n",
    "- export all transactions `X`:\n",
    "    - X is 2 hops away from a ww2 coinjoin\n",
    "    - all inputs to X are from a WW2 coinjoin\n",
    "- output the same as `wasabi2_events.json`\n",
    "Structure:\n",
    "- txid\n",
    "    - txid\n",
    "    - block_index\n",
    "    - broadcast_time\n",
    "    - inputs\n",
    "        - input number\n",
    "            - value\n",
    "            - wallet_name\n",
    "            - mix_event_type\n",
    "    - outputs\n",
    "        - output number\n",
    "            - value\n",
    "            - wallet_name\n",
    "            - mix_event_type\n",
    "    - num_inputs\n",
    "    - num_outputs\n",
    "    \n",
    "# !!!!!!! OUTPUT IS SPENT IN SPENDING TX !!!!!!!\n",
    "# !!!!!!! INPUT WAS SPENT IN SPENT TX !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb702bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = next(iter(wasabi2_events.keys()))\n",
    "key_tx = chain.tx_with_hash(key)\n",
    "first_next = key_tx.outputs[0].spending_tx\n",
    "print(key_tx.hash)\n",
    "print(first_next.hash)\n",
    "print(first_next.outputs[0].spending_tx.hash)\n",
    "print(first_next.outputs[0].spending_tx.inputs[0].spent_tx.hash)\n",
    "print(next(iter(wasabi2_events.values()))[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# outputs list of transactions where the condition holds\n",
    "def find_friends_do_not_pay_txes(tx: blocksci.Tx, **kwargs) -> List[str]:\n",
    "    ww2_events = kwargs[\"ww2_events\"]\n",
    "    result = []\n",
    "    for out1 in tx.outputs:\n",
    "        if not out1.is_spent:\n",
    "            continue\n",
    "        \n",
    "        for out2 in out1.spending_tx.outputs:\n",
    "            if not out2.is_spent:\n",
    "                continue\n",
    "            \n",
    "            if not out2.spending_tx.hash in ww2_events:\n",
    "                continue\n",
    "            \n",
    "            curr = out2.spending_tx\n",
    "            for inp in curr.inputs:\n",
    "                if str(inp.spent_tx.hash) not in ww2_events:\n",
    "                    break\n",
    "            else:\n",
    "                result.append(str(curr.hash))\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "%time friends = chain.map_spliterator(map_func=find_friends_do_not_pay_txes, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory),ww2_events=wasabi2_events,workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ae25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop = get_block_height_range('2023-01-01', '2023-02-01')\n",
    "%time friends = chain.find_friends_who_dont_pay(keys=wasabi2_events, start=0, stop=len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "def process_inputs(tx: blocksci.Tx) -> List[Dict[str, Any]]:\n",
    "    res = []\n",
    "    inp: blocksci.Input\n",
    "    for inp in tx.inputs:\n",
    "        spent_tx: blocksci.Tx = inp.spent_tx\n",
    "        imm = {\n",
    "            str(inp.index): {\n",
    "                \"value\": inp.value,\n",
    "                \"wallet_name\": inp.address,\n",
    "                \"is_ww2_coinjoin\": str(inp.spent_tx.hash) in wasabi2_events\n",
    "            }\n",
    "        }\n",
    "        res.append(imm)\n",
    "        \n",
    "    return res\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def process_outputs(tx: blocksci.Tx) -> List[Dict[str, Any]]:\n",
    "    res = []\n",
    "    out: blocksci.Output\n",
    "    for out in tx.outputs:\n",
    "        imm = {\n",
    "            str(out.index): {\n",
    "                \"value\": out.value,\n",
    "                \"wallet_name\": out.address,\n",
    "                \"is_ww2_coinjoin\": out.is_spent and str(out.spending_tx.hash) in wasabi2_events\n",
    "            }\n",
    "        }\n",
    "        res.append(imm)\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def fill_json_info(tx: blocksci.Tx) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"txid\": str(tx.hash),\n",
    "        \"block_index\": str(tx.block_height),\n",
    "        \"broadcast_time\": tx.block_time,\n",
    "        \"num_inputs\": tx.input_count,\n",
    "        \"num_outputs\": tx.output_count,\n",
    "        \"inputs\": process_inputs(tx),\n",
    "        \"outputs\": process_outputs(tx),\n",
    "    }\n",
    "\n",
    "result = {}\n",
    "\n",
    "for tx_id in friends:\n",
    "    tx = chain.tx_with_hash(tx_id)\n",
    "    result[tx_id] = fill_json_info(tx)\n",
    "    \n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b146286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/mnt/anal/ww2_fdnp.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c46bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/anal/ww2_fdnp.json', 'r') as f:\n",
    "    res = json.load(f)\n",
    "    \n",
    "for tx, val in res.items():\n",
    "    for s in val[\"inputs\"]:\n",
    "        for inp, vals in s.items():\n",
    "            if vals['is_ww2_coinjoin']:\n",
    "                print(tx, inp, vals)\n",
    "                break\n",
    "                \n",
    "                \n",
    "# tx = friends[0]\n",
    "# tx = chain.tx_with_hash(tx)\n",
    "\n",
    "# print(str(tx.inputs[0].spent_tx.hash) in wasabi2_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ff822",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time one_input_output_txes = chain.find_hw_sw_coinjoins(start=len(chain) - 190, stop=len(chain))\n",
    "print(len(one_input_output_txes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cb7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(one_input_output_txes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7e9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830606\n"
     ]
    }
   ],
   "source": [
    "print(len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc243d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 56s, sys: 1.2 s, total: 13min 57s\n",
      "Wall time: 4.55 s\n",
      "CPU times: user 17min 54s, sys: 6.06 s, total: 18min\n",
      "Wall time: 5.89 s\n",
      "CPU times: user 14min 6s, sys: 2.94 s, total: 14min 9s\n",
      "Wall time: 4.64 s\n",
      "CPU times: user 17min 46s, sys: 4.17 s, total: 17min 50s\n",
      "Wall time: 5.8 s\n",
      "CPU times: user 13min 54s, sys: 195 ms, total: 13min 54s\n",
      "Wall time: 4.28 s\n",
      "CPU times: user 14min 7s, sys: 2.17 s, total: 14min 10s\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%time w2_wp = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi2_events, whirlpool_events, True)\n",
    "%time wp_w2 = chain.find_traverses_in_coinjoin_flows(0, len(chain), whirlpool_events, wasabi2_events, True)\n",
    "%time w1_wp = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi_events, whirlpool_events, True)\n",
    "%time wp_w1 = chain.find_traverses_in_coinjoin_flows(0, len(chain), whirlpool_events, wasabi_events, True)\n",
    "%time w2_w1 = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi2_events, wasabi_events, True)\n",
    "%time w1_w2 = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi_events, whirlpool_events, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34229a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasabi2 -> Whirlpool Total length: 48, case 1: 0, case 2: 48\n",
      "Whirlpool -> Wasabi2 Total length: 1386, case 1: 855, case 2: 531\n",
      "Wasabi -> Whirlpool Total length: 5, case 1: 1, case 2: 4\n",
      "Whirlpool -> Wasabi Total length: 598, case 1: 337, case 2: 261\n",
      "Wasabi2 -> Wasabi Total length: 53, case 1: 43, case 2: 10\n",
      "Wasabi -> Wasabi2 Total length: 5, case 1: 1, case 2: 4\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Wasabi2 -> Whirlpool\", \"Whirlpool -> Wasabi2\", \"Wasabi -> Whirlpool\", \"Whirlpool -> Wasabi\", \"Wasabi2 -> Wasabi\", \"Wasabi -> Wasabi2\"]\n",
    "results = [w2_wp, wp_w2, w1_wp, wp_w1, w2_w1, w1_wp]\n",
    "\n",
    "for label, result in zip(labels, results):\n",
    "    print(label, f\"Total length: {len(result)}, case 1: {len(list(filter(lambda x: x[4], result)))}, case 2: {len(list(filter(lambda x: not x[4], result)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463d134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e7d3bbde6c489e97141c8a2c3b13f1b83e2e5d80dc74c254b0bed8763d68ddf1', {'d31c2b4d71eb143b23bb87919dda7fdfecee337ffa1468d1c431ece37698f918'}, {'adb90a7f1e3fb7dc2abf94727d783152ee24ace63c9c234ed69a8701b57a7c09'}, 100302, False)\n"
     ]
    }
   ],
   "source": [
    "# 2024-05-04 18:08:36,618 - INFO - Whirlpool -> Wasabi2: Num txs between mix1 and mix2: 1545\n",
    "# 2024-05-04 18:08:36,791 - INFO - Wasabi1 -> Whirlpool: Num txs between mix1 and mix2: 27\n",
    "# 2024-05-04 18:08:37,042 - INFO - Wasabi1 -> Wasabi2: Num txs between mix1 and mix2: 8548\n",
    "# 2024-05-04 18:08:37,122 - INFO - Wasabi2 -> Whirlpool: Num txs between mix1 and mix2: 42\n",
    "# 2024-05-04 18:08:37,250 - INFO - Wasabi2 -> Wasabi1: Num txs between mix1 and mix2: 789\n",
    "\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
