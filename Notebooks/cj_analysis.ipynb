{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca445310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blocksci\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "parser_data_directory = Path(\"/mnt/anal/config.json\")\n",
    "cluster_directory = Path(\"/mnt/anal/cluster/\")\n",
    "dumplings_directory = Path(\"/mnt/dumplings/\")\n",
    "\n",
    "chain = blocksci.Blockchain(str(parser_data_directory))\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def get_block_height_for_date(date: str) -> int:\n",
    "    return chain.range(date)[0].height\n",
    "\n",
    "def get_block_height_range(start: str, end: str) -> Tuple[int, int]:\n",
    "    return get_block_height_for_date(start), get_block_height_for_date(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "wasabi2_events_file = dumplings_directory / \"wasabi2_events.json\"\n",
    "# wasabi2_txs_file = dumplings_directory / \"wasabi2_txs.json\"\n",
    "wasabi_events_file = dumplings_directory / \"wasabi1_events.json\"\n",
    "# wasabi_txs_file = dumplings_directory / \"wasabi_txs.json\"\n",
    "whirlpool_events_file = dumplings_directory / \"whirlpool_events.json\"\n",
    "# whirlpool_txs_file = dumplings_directory / \"whirlpool_txs.json\"\n",
    "\n",
    "with open(wasabi2_events_file) as f:\n",
    "    wasabi2_events = json.load(f)\n",
    "\n",
    "# with open(wasabi2_txs_file) as f:\n",
    "#     wasabi2_txs = json.load(f)\n",
    "\n",
    "with open(wasabi_events_file) as f:\n",
    "    wasabi_events = json.load(f)\n",
    "\n",
    "# with open(wasabi_txs_file) as f:\n",
    "#     wasabi_txs = json.load(f)\n",
    "\n",
    "with open(whirlpool_events_file) as f:\n",
    "    whirlpool_events = json.load(f)\n",
    "\n",
    "# with open(whirlpool_txs_file) as f:\n",
    "#     whirlpool_txs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for txid in wasabi2_events.keys():\n",
    "    try:\n",
    "        tx = chain.tx_with_hash(txid)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    v = tx.is_ww2_coinjoin()\n",
    "    if not v:\n",
    "#         print(tx)\n",
    "        pass\n",
    "    else:\n",
    "        count += 1\n",
    "    inp: blocksci.Input\n",
    "        \n",
    "# print(count, count - len(wasabi2_events))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time filtered_ww2_coinjoins = chain.filter_ww2_coinjoin_txes(0, len(chain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1089b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_coinjoins_found = set(map(lambda x: str(x.hash), filtered_ww2_coinjoins))\n",
    "res = {'only_in_dumplings': [], 'only_in_blocksci': [], 'in_both': []}\n",
    "\n",
    "for tx in filtered_ww2_coinjoins:\n",
    "    if str(tx.hash) in ww2_coinjoins_found and str(tx.hash) in wasabi2_events:\n",
    "        res['in_both'].append(tx)\n",
    "    elif str(tx.hash) in ww2_coinjoins_found:\n",
    "        res['only_in_blocksci'].append(tx)\n",
    "        \n",
    "for tx in wasabi2_events.keys():\n",
    "    if tx not in ww2_coinjoins_found:\n",
    "        res['only_in_dumplings'].append(tx)\n",
    "        \n",
    "print(f\"Only in dumplings: {len(res['only_in_dumplings'])}, only in blocksci: {len(res['only_in_blocksci'])}, in both: {len(res['in_both'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210d4e4",
   "metadata": {},
   "source": [
    "# Basic analysis\n",
    "Here are some basic analyses for the coinjoins just to make sure nothign fishy is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(tx: blocksci.Tx) -> Tuple[int, int]:\n",
    "    return (tx.input_count, tx.output_count)\n",
    "\n",
    "def find_number_of_unique_txs(tx: blocksci.Tx) -> Tuple[int, int]:\n",
    "    return len(set([x.value for x in tx.inputs])), len(set([x.value for x in tx.outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = chain.map_spliterator(map_func=find_number_of_unique_txs, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e295e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "\n",
    "flattened_unique_inputs = [x[0] for y in unique_counts for x in y]\n",
    "flattened_unique_outputs = [x[1] for y in unique_counts for x in y]\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(f\"min: {min(flattened_unique_inputs)}, max: {max(flattened_unique_inputs)}, median: {median(flattened_unique_inputs)}\")\n",
    "\n",
    "print(\"outputs:\")\n",
    "print(f\"min: {min(flattened_unique_outputs)}, max: {max(flattened_unique_outputs)}, median: {median(flattened_unique_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxes = chain.map_spliterator(map_func=find_min_max, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, out = minmaxes[0][0]\n",
    "\n",
    "for x in minmaxes:\n",
    "    for input1, output1 in x:\n",
    "        if input1 > inp:\n",
    "            inp = input1\n",
    "        if output1 > out:\n",
    "            out = output1\n",
    "            \n",
    "print(inp, out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42483ad5",
   "metadata": {},
   "source": [
    "# Remix analysis\n",
    "\n",
    "Here we have the map functions for different remix analyses. The functions should have the following interface\n",
    "`map_func(tx: blocksci.Tx, **kwargs) -> T`\n",
    "where `T` is the common result type. The result will be added to the list of results of each worker.\n",
    "\n",
    "`kwargs` are the arguments passed to each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_remixes_within_one_hop(tx: blocksci.Tx, **kwargs):\n",
    "    \"\"\"Pass in `events` as kwarg. Will check whether there is an output of `tx` in `events`.\n",
    "    We can count this as 'remix' transaction.\n",
    "    \"\"\"\n",
    "    cj_events = kwargs['events']\n",
    "    result = (tx.hash, tx.output_count, [])\n",
    "    for c, i in enumerate(tx.outputs):\n",
    "        if not i.is_spent:\n",
    "            continue\n",
    "        \n",
    "        if str(i.spending_tx.hash) in cj_events:\n",
    "            result[2].append((i.index, True))\n",
    "            \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb68fa",
   "metadata": {},
   "source": [
    "### Results processing\n",
    "\n",
    "We take the outputs of the above functions and compute various statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b80083",
   "metadata": {},
   "outputs": [],
   "source": [
    "wasabi2_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi2_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi2_events.keys())\n",
    "wasabi_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi_events.keys())\n",
    "whirlpool_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(whirlpool_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=128, events=whirlpool_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835379c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=64, events=whirlpool_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f91015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_results = chain.map_spliterator(find_remixes_within_one_hop, list(wasabi2_events.keys()), str(parser_data_directory), workers=64, events=wasabi2_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_remix_stats(results, events, key):\n",
    "    stats_computed = {\"remix\": 0, \"left\": 0}\n",
    "    \n",
    "    for one in results:\n",
    "        for txid, all_outputs, actual in one:\n",
    "            stats_computed[\"remix\"] += len(actual)\n",
    "            stats_computed[\"left\"] += all_outputs - len(actual)\n",
    "    \n",
    "    print(key)\n",
    "    print(stats_computed)\n",
    "\n",
    "    dumplings_stats = {\"dumplings_remix\": 0}\n",
    "\n",
    "    for tx in events.values():\n",
    "        for out in tx[\"outputs\"].values():\n",
    "            if out[\"mix_event_type\"] == \"MIX_REMIX\":\n",
    "                dumplings_stats[\"dumplings_remix\"] += 1\n",
    "\n",
    "    print(dumplings_stats)\n",
    "    print(f\"dumplings - computed: {dumplings_stats['dumplings_remix'] - stats_computed['remix']}\\n\")\n",
    "\n",
    "compute_remix_stats(wasabi2_results, wasabi2_txs[\"coinjoins\"], \"wasabi2 1 hop\")\n",
    "compute_remix_stats(wasabi_results, wasabi_txs[\"coinjoins\"], \"wasabi 1 hop\")\n",
    "compute_remix_stats(whirlpool_results, whirlpool_txs[\"coinjoins\"], \"whirlpool 1 hop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02838a96",
   "metadata": {},
   "source": [
    "# Consolidation analysis\n",
    "\n",
    "The functions for consolidation analysis are here. The interface is still the same:\n",
    "`map_func(tx: blocksci.Tx, **kwargs) -> list[T]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf071fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def find_outputs_one_hop(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        output_spent_in = str(output.spending_tx.hash)\n",
    "        if output_spent_in not in found:\n",
    "            found[output_spent_in] = 0\n",
    "        found[output_spent_in] += 1\n",
    "\n",
    "\n",
    "\n",
    "def find_consolidation(tx: blocksci.Tx, **kwargs) -> List[Tuple[str, Dict[str, List[str]]]]:\n",
    "    found_for_tx = {}\n",
    "    find_outputs_one_hop(tx, found_for_tx)\n",
    "    return str(tx.hash), found_for_tx\n",
    "\n",
    "\n",
    "def has_tx_output_in_coinjoin_events(tx: blocksci.Tx, **kwargs) -> bool:\n",
    "    cj_events = kwargs[\"events\"]\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        if str(output.spending_tx.hash) in cj_events:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_outputs_two_hops(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        if output.spending_tx.output_count < 2:\n",
    "            output_spent_in = str(output.spending_tx.hash)\n",
    "            if output_spent_in not in found:\n",
    "                found[output_spent_in] = 0\n",
    "            found[output_spent_in] += 1\n",
    "            continue\n",
    "\n",
    "        for output2 in output.spending_tx.outputs:\n",
    "            if not output2.is_spent:\n",
    "                continue\n",
    "                \n",
    "            if output2.spending_tx.output_count < 2:\n",
    "                output_spent_in = str(output2.spending_tx.hash)\n",
    "                if output_spent_in not in found:\n",
    "                    found[output_spent_in] = 0\n",
    "                found[output_spent_in] += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "def find_outputs_three_hops(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "            \n",
    "        found_match = False\n",
    "\n",
    "        if output.spending_tx.output_count < 2:\n",
    "            output_spent_in = str(output.spending_tx.hash)\n",
    "            if output_spent_in not in found:\n",
    "                found[output_spent_in] = 0\n",
    "            found[output_spent_in] += 1\n",
    "            continue\n",
    "\n",
    "        for output2 in output.spending_tx.outputs:\n",
    "            if found_match:\n",
    "                break\n",
    "            if not output2.is_spent:\n",
    "                continue\n",
    "                \n",
    "            if output2.spending_tx.output_count < 2:\n",
    "                output_spent_in = str(output2.spending_tx.hash)\n",
    "                if output_spent_in not in found:\n",
    "                    found[output_spent_in] = 0\n",
    "                found[output_spent_in] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "            \n",
    "            for output3 in output2.spending_tx.outputs:\n",
    "                if not output3.is_spent:\n",
    "                    continue\n",
    "\n",
    "                if output3.spending_tx.output_count < 2:\n",
    "                    output_spent_in = str(output3.spending_tx.hash)\n",
    "                    if output_spent_in not in found:\n",
    "                        found[output_spent_in] = 0\n",
    "                    found[output_spent_in] += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "                    \n",
    "def find_consolidation_three_hops(tx: blocksci.Tx):\n",
    "    found_for_tx = {}\n",
    "    find_outputs_three_hops(tx, found_for_tx)\n",
    "    return (str(tx.hash), found_for_tx)\n",
    "\n",
    "\n",
    "def find_consolidation_two_hops(tx: blocksci.Tx):\n",
    "    found_for_tx = {}\n",
    "    find_outputs_two_hops(tx, found_for_tx)\n",
    "    return (str(tx.hash), found_for_tx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)\n",
    "# %time wasabi_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation = chain.map_spliterator(map_func=find_consolidation, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation = chain.map_spliterator(map_func=find_consolidation, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cb1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_consolidation = chain.map_spliterator(map_func=find_consolidation, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095695bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ed9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea7b7b",
   "metadata": {},
   "source": [
    "### Consolidation analyses\n",
    "\n",
    "Following are the functions computing the actual result analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57a7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_consolidated_txes_in_one_hop(consolidated):\n",
    "    total_outgoing = 0\n",
    "    counts_of_consolidated_txes_in_one_hop = defaultdict(int)\n",
    "    for root_tx, dct in consolidated:\n",
    "            for outbound_tx, count in dct.items():\n",
    "                if outbound_tx in wasabi2_events or outbound_tx in wasabi_events or outbound_tx in whirlpool_events:\n",
    "                    continue\n",
    "\n",
    "                counts_of_consolidated_txes_in_one_hop[count] += 1\n",
    "    return counts_of_consolidated_txes_in_one_hop\n",
    "\n",
    "\n",
    "# w2_consolidation_1hop = compute_consolidated_txes_in_one_hop(wasabi2_consolidation)\n",
    "# w_consolidation_1hop = compute_consolidated_txes_in_one_hop(wasabi_consolidation)\n",
    "# whirl_consolidation_1hop = compute_consolidated_txes_in_one_hop(whirlpool_consolidation)\n",
    "\n",
    "def plot_barplot_from_dictionary(dct):\n",
    "    plt.bar(range(len(dct)), list(dct.values()), align='center')\n",
    "    plt.xticks(range(len(dct)), list(dct.keys()))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def make_graph(data, name):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(8, 9))\n",
    "\n",
    "    # Plot the data for 2-8 on the first subplot\n",
    "    ax1.bar(list(range(2, 9)), [data[key] for key in range(2, 9)])\n",
    "    ax1.set_xlabel('X-axis')\n",
    "    ax1.set_ylabel('Y-axis (2-8)')\n",
    "    ax1.set_title(f'{name}: Bar Plot (2-8)')\n",
    "\n",
    "    # Plot the data for 9-15 on the second subplot\n",
    "    ax2.bar(list(range(9, 16)), [data[key] for key in range(9, 16)])\n",
    "    ax2.set_xlabel('X-axis')\n",
    "    ax2.set_ylabel('Y-axis (9-15)')\n",
    "    ax2.set_title(f'{name}: Bar Plot (9-15)')\n",
    "\n",
    "    ax3.bar(list(range(16, 23)), [data[key] for key in range(16, 23)])\n",
    "    ax3.set_xlabel('X-axis')\n",
    "    ax3.set_ylabel('Y-axis (16-22)')\n",
    "    ax3.set_title(f'{name}: Bar Plot (16-22)')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# print(f\"w2 unpaired: {w2_consolidation_1hop[1]}\")\n",
    "# print(f\"w unpaired: {w_consolidation_1hop[1]}\")\n",
    "# print(f\"whirlpool unpaired: {whirl_consolidation_1hop[1]}\")\n",
    "\n",
    "# make_graph(w2_consolidation_1hop, \"wasabi2\")\n",
    "# make_graph(w_consolidation_1hop, \"wasabi\")\n",
    "# make_graph(whirl_consolidation_1hop, \"whirlpool\")\n",
    "\n",
    "\n",
    "\n",
    "# w2_2hop_consolidated = compute_consolidated_txes_in_one_hop(wasabi2_consolidation_two_hop)\n",
    "# w_2hop_consolidated = compute_consolidated_txes_in_one_hop(wasabi_consolidation_two_hop)\n",
    "# wh_2hop_consolidated = compute_consolidated_txes_in_one_hop(whirlpool_consolidation_two_hop)\n",
    "\n",
    "\n",
    "# make_graph(w2_2hop_consolidated, \"two hop consolidation to one output wasabi2\")\n",
    "# make_graph(w_2hop_consolidated, \"two hop consolidation to one output wasabi\")\n",
    "# make_graph(wh_2hop_consolidated, \"two hop consolidation to one output whirlpool\")\n",
    "\n",
    "print(len(res_w2))\n",
    "w2_consolidation_3hop = compute_consolidated_txes_in_one_hop(res_w2)\n",
    "w_consolidation_3hop = compute_consolidated_txes_in_one_hop(res_w1)\n",
    "\n",
    "\n",
    "print(f\"w2 unpaired: {w2_consolidation_3hop[1]}\")\n",
    "print(f\"w unpaired: {w_consolidation_3hop[1]}\")\n",
    "\n",
    "\n",
    "make_graph(w2_consolidation_3hop, \"wasabi2\")\n",
    "make_graph(w_consolidation_3hop, \"wasabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e2939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'\n",
    "consolidated_tx = '0abebd6704fcd886b1e74815ce05a24a11aa2d0e543729d6dbd18629c72874a7'\n",
    "\n",
    "in_tx = chain.tx_with_hash(in_tx)\n",
    "consolidated_tx = chain.tx_with_hash(consolidated_tx) \n",
    "\n",
    "print(consolidated_tx.output_count)\n",
    "\n",
    "\n",
    "from typing import Optional, Set\n",
    "\n",
    "consolidated_txs_lost = list(find_consolidation_two_hops(in_tx))[1]\n",
    "\n",
    "# print(consolidated_txs)\n",
    "consolidated_txs = [x for x, y in consolidated_txs_lost.items() if y < 10]\n",
    "print(len(consolidated_txs), consolidated_txs_lost[consolidated_txs[0]])\n",
    "\n",
    "def subset_sum_rec(nums: List[int], total: int, start: int, memo: Dict[Tuple[int, int], Optional[Set[int]]]) -> Optional[Set[int]]:\n",
    "    if total == 0:\n",
    "        return set()\n",
    "\n",
    "    if start == len(nums):\n",
    "        return None\n",
    "\n",
    "    key = (start, total)\n",
    "    if key in memo:\n",
    "        return memo[key]\n",
    "\n",
    "    num = nums[start]\n",
    "    if num.value > total:\n",
    "        memo[key] = None\n",
    "        return None\n",
    "\n",
    "    result = subset_sum_rec(nums, total - num.value, start + 1, memo)\n",
    "    if result is not None:\n",
    "        result.add(num)\n",
    "        memo[key] = result\n",
    "        return result\n",
    "\n",
    "    result = subset_sum_rec(nums, total, start + 1, memo)\n",
    "    memo[key] = result\n",
    "    return result\n",
    "\n",
    "def find_sum_candidates(tx, inputs, output_value):\n",
    "    memo = {}\n",
    "    sorted_inputs = sorted(filter(lambda y: y.value <= output_value, inputs), key=lambda x: x.value)\n",
    "    return subset_sum_rec(sorted_inputs, output_value, 0, memo)\n",
    "\n",
    "a = set()\n",
    "for i in range(3):\n",
    "    inputs = set(in_tx.inputs) - a\n",
    "    value = chain.tx_with_hash(consolidated_txs[i]).outputs[0].value\n",
    "    r = find_sum_candidates(in_tx, inputs, value)\n",
    "    if r:\n",
    "        r = a\n",
    "\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'\n",
    "consolidated_tx = '0abebd6704fcd886b1e74815ce05a24a11aa2d0e543729d6dbd18629c72874a7'\n",
    "\n",
    "in_tx = chain.tx_with_hash(in_tx)\n",
    "consolidated_tx = chain.tx_with_hash(consolidated_tx) \n",
    "\n",
    "print(in_tx)\n",
    "print(consolidated_tx.inputs[0])\n",
    "print(consolidated_tx.inputs[0].spent_output.address_type)\n",
    "\n",
    "print(in_tx.block.timestamp)\n",
    "print(in_tx.outputs[0].block.timestamp)\n",
    "print(in_tx.outputs[0].spending_tx.block.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fecef9",
   "metadata": {},
   "source": [
    "# embrace VUT\n",
    "\n",
    "Let's try to use some analyses with [coinomon](https://coinomon.bazar.nesad.fit.vutbr.cz/#/Authentication/login).\n",
    "\n",
    "- get all coinjoins in one month (say Feb 23)\n",
    "- get all output txs from them (with one output)\n",
    "- pick one randomly and get some data about them from coinomon\n",
    "- ???\n",
    "- profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d258f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CoinomonClient:\n",
    "    def __init__(self, token: str) -> None:\n",
    "        self.crypto = \"BTC\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        self.base_url = \"https://coinomon.bazar.nesad.fit.vutbr.cz/\"\n",
    "    \n",
    "    def get_address_info(self, address: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"{self.base_url}jwt/v1/{self.crypto}/cryptoaddress/{address}/summary\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response.text))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    \n",
    "    def get_cluster_info(self, cluster_id: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"/jwt/v1/{self.crypto}/cryptocluster/{cluster_id}\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    def get_cluster_addresses(self, cluster_id: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"/jwt/v1/{self.crypto}/cryptocluster/{cluster_id}/addresses\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "token = \"insert here\"\n",
    "\n",
    "coinomon_client = CoinomonClient(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(chain.filter_in_keys)\n",
    "%time txes = chain.filter_in_keys(wasabi2_events, 0, len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568926c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e622f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 35s, sys: 15min 11s, total: 21min 47s\n",
      "Wall time: 9.05 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# start, end = get_block_height_range('2023-02-01', '2023-02-28')\n",
    "# %time res = chain.find_consolidation_3_hops(wasabi2_events, start, end)\n",
    "# %time res_w1 = chain.find_consolidation_3_hops(wasabi_events, 0, len(chain))\n",
    "%time res_w2 = chain.find_consolidation_3_hops(wasabi2_events, 0, len(chain))\n",
    "# %time res_wh = chain.find_consolidation_3_hops(whirlpool_events, 0, len(chain))\n",
    "\n",
    "# %time res = chain.find_consolidation_3_hops(whirlpool_events, 774513, 778584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea94e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "max_outputs = \"\", \"\", 0\n",
    "            \n",
    "for tx, outputs in res:\n",
    "    for out, val in outputs.items():\n",
    "        if val > max_outputs[2]:\n",
    "            max_outputs = out, tx, val\n",
    "\n",
    "print(len(res))\n",
    "print(max_outputs)\n",
    "tx = chain.tx_with_hash(max_outputs[0])\n",
    "coinomon_data = coinomon_client.get_address_info(tx.outputs[0].address.address_string)\n",
    "coinomon_data[\"data\"].pop(\"firstTx\")\n",
    "coinomon_data[\"data\"].pop(\"lastTx\")\n",
    "print(json.dumps(coinomon_data, indent=4))\n",
    "\n",
    "tx_start = chain.tx_with_hash(max_outputs[1])\n",
    "\n",
    "coinomon_data = coinomon_client.get_address_info(tx_start.outputs[0].address.address_string)\n",
    "coinomon_data[\"data\"].pop(\"firstTx\")\n",
    "coinomon_data[\"data\"].pop(\"lastTx\")\n",
    "print(json.dumps(coinomon_data, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aaf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx, _ in res:\n",
    "    tx = chain.tx_with_hash(tx)\n",
    "    coinomon_data = coinomon_client.get_address_info(tx.outputs[0].address.address_string)\n",
    "    if coinomon_data[\"data\"][\"alarms\"]:\n",
    "        print(coinomon_data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56643b19",
   "metadata": {},
   "source": [
    "# Juralysis\n",
    "\n",
    "Try some BlockSci clustering on some coinjoin output.\n",
    "Let's pick the same way as before - Feb2023, one with largest tx count (why not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac13736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time jura_res = chain.find_consolidation_3_hops(wasabi_events, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d931e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_heuristic = blocksci.heuristics.change.legacy\n",
    "\n",
    "cm = blocksci.cluster.ClusterManager.create_clustering(\"/mnt/anal/cluster\", chain, heuristic=legacy_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "jura_tx = chain.tx_with_hash(list(jura_res[0][1].keys())[0]) # don't ask\n",
    "cluster = cm.cluster_with_address(jura_tx.outputs[0].address)\n",
    "print(cluster.address_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in cm.clusters().to_list():\n",
    "    cnt += 1\n",
    "    \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = chain.tx_with_hash(res_w2[-3][0]).outputs\n",
    "m_o = map(lambda x: x.value, outputs)\n",
    "print(list(sorted(m_o, reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "outputs = {}\n",
    "\n",
    "for k in wasabi2_events:\n",
    "    try:\n",
    "        tx = chain.tx_with_hash(k)\n",
    "    except Exception:\n",
    "#         print(k, \"not found\")\n",
    "        pass\n",
    "    for out in tx.outputs:\n",
    "        value = out.value\n",
    "        if value not in outputs:\n",
    "            outputs[value] = 0\n",
    "        outputs[value] += 1\n",
    "    \n",
    "# import json\n",
    "# print(json.dumps(outputs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eaa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = list(sorted(outputs.items(), key=lambda x: -x[1]))\n",
    "a = {k: v for k, v in f}\n",
    "# print(json.dumps(a, indent=4))\n",
    "print(len(list(filter(lambda x: x[1] > 10, a.items()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e533de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = [5000, 6561, 8192, 10000, 13122, 16384, 19683, 20000, 32768, 39366, 50000, 59049, 65536, 100000, 118098, 131072,\n",
    " 177147, 200000, 262144, 354294, 500000, 524288, 531441, 1000000, 1048576, 1062882, 1594323, 2000000, 2097152,\n",
    "3188646, 4194304, 4782969, 5000000, 8388608, 9565938, 10000000, 14348907, 16777216, 20000000, 28697814, 33554432,\n",
    "43046721, 50000000, 67108864, 86093442, 100000000, 129140163, 134217728, 200000000, 258280326, 268435456, 387420489,\n",
    "500000000, 536870912, 774840978, 1000000000, 1073741824, 1162261467, 2000000000, 2147483648, 2324522934, 3486784401,\n",
    "4294967296, 5000000000, 6973568802, 8589934592, 10000000000, 10460353203, 17179869184, 20000000000, 20920706406,\n",
    "31381059609, 34359738368, 50000000000, 62762119218, 68719476736, 94143178827, 100000000000, 137438953472]\n",
    "\n",
    "\n",
    "for i in common:\n",
    "    print(f\"{i}: {a.get(i)}\")\n",
    "# print(a[10000000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90d58e",
   "metadata": {},
   "source": [
    "# Analysis over the coinjoins (friends do not pay)\n",
    "**(petrs request)**\n",
    "\n",
    "- export all transactions `X`:\n",
    "    - X is 2 hops away from a ww2 coinjoin\n",
    "    - all inputs to X are from a WW2 coinjoin\n",
    "- output the same as `wasabi2_events.json`\n",
    "Structure:\n",
    "- txid\n",
    "    - txid\n",
    "    - block_index\n",
    "    - broadcast_time\n",
    "    - inputs\n",
    "        - input number\n",
    "            - value\n",
    "            - wallet_name\n",
    "            - mix_event_type\n",
    "    - outputs\n",
    "        - output number\n",
    "            - value\n",
    "            - wallet_name\n",
    "            - mix_event_type\n",
    "    - num_inputs\n",
    "    - num_outputs\n",
    "    \n",
    "# !!!!!!! OUTPUT IS SPENT IN SPENDING TX !!!!!!!\n",
    "# !!!!!!! INPUT WAS SPENT IN SPENT TX !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb702bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = next(iter(wasabi2_events.keys()))\n",
    "key_tx = chain.tx_with_hash(key)\n",
    "first_next = key_tx.outputs[0].spending_tx\n",
    "print(key_tx.hash)\n",
    "print(first_next.hash)\n",
    "print(first_next.outputs[0].spending_tx.hash)\n",
    "print(first_next.outputs[0].spending_tx.inputs[0].spent_tx.hash)\n",
    "print(next(iter(wasabi2_events.values()))[\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# outputs list of transactions where the condition holds\n",
    "def find_friends_do_not_pay_txes(tx: blocksci.Tx, **kwargs) -> List[str]:\n",
    "    ww2_events = kwargs[\"ww2_events\"]\n",
    "    result = []\n",
    "    for out1 in tx.outputs:\n",
    "        if not out1.is_spent:\n",
    "            continue\n",
    "        \n",
    "        for out2 in out1.spending_tx.outputs:\n",
    "            if not out2.is_spent:\n",
    "                continue\n",
    "            \n",
    "            if not out2.spending_tx.hash in ww2_events:\n",
    "                continue\n",
    "            \n",
    "            curr = out2.spending_tx\n",
    "            for inp in curr.inputs:\n",
    "                if str(inp.spent_tx.hash) not in ww2_events:\n",
    "                    break\n",
    "            else:\n",
    "                result.append(str(curr.hash))\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "%time friends = chain.map_spliterator(map_func=find_friends_do_not_pay_txes, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory),ww2_events=wasabi2_events,workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ae25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop = get_block_height_range('2023-01-01', '2023-02-01')\n",
    "%time friends = chain.find_friends_who_dont_pay(keys=wasabi2_events, start=0, stop=len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "def process_inputs(tx: blocksci.Tx) -> List[Dict[str, Any]]:\n",
    "    res = []\n",
    "    inp: blocksci.Input\n",
    "    for inp in tx.inputs:\n",
    "        spent_tx: blocksci.Tx = inp.spent_tx\n",
    "        imm = {\n",
    "            str(inp.index): {\n",
    "                \"value\": inp.value,\n",
    "                \"wallet_name\": inp.address,\n",
    "                \"is_ww2_coinjoin\": str(inp.spent_tx.hash) in wasabi2_events\n",
    "            }\n",
    "        }\n",
    "        res.append(imm)\n",
    "        \n",
    "    return res\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def process_outputs(tx: blocksci.Tx) -> List[Dict[str, Any]]:\n",
    "    res = []\n",
    "    out: blocksci.Output\n",
    "    for out in tx.outputs:\n",
    "        imm = {\n",
    "            str(out.index): {\n",
    "                \"value\": out.value,\n",
    "                \"wallet_name\": out.address,\n",
    "                \"is_ww2_coinjoin\": out.is_spent and str(out.spending_tx.hash) in wasabi2_events\n",
    "            }\n",
    "        }\n",
    "        res.append(imm)\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def fill_json_info(tx: blocksci.Tx) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"txid\": str(tx.hash),\n",
    "        \"block_index\": str(tx.block_height),\n",
    "        \"broadcast_time\": tx.block_time,\n",
    "        \"num_inputs\": tx.input_count,\n",
    "        \"num_outputs\": tx.output_count,\n",
    "        \"inputs\": process_inputs(tx),\n",
    "        \"outputs\": process_outputs(tx),\n",
    "    }\n",
    "\n",
    "result = {}\n",
    "\n",
    "for tx_id in friends:\n",
    "    tx = chain.tx_with_hash(tx_id)\n",
    "    result[tx_id] = fill_json_info(tx)\n",
    "    \n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b146286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/mnt/anal/ww2_fdnp.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c46bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/anal/ww2_fdnp.json', 'r') as f:\n",
    "    res = json.load(f)\n",
    "    \n",
    "for tx, val in res.items():\n",
    "    for s in val[\"inputs\"]:\n",
    "        for inp, vals in s.items():\n",
    "            if vals['is_ww2_coinjoin']:\n",
    "                print(tx, inp, vals)\n",
    "                break\n",
    "                \n",
    "                \n",
    "# tx = friends[0]\n",
    "# tx = chain.tx_with_hash(tx)\n",
    "\n",
    "# print(str(tx.inputs[0].spent_tx.hash) in wasabi2_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5ff822",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'blocksci.Blockchain' object has no attribute 'find_hw_sw_coinjoins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'blocksci.Blockchain' object has no attribute 'find_hw_sw_coinjoins'"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'one_input_output_txes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_input_output_txes = chain.find_hw_sw_coinjoins(start=len(chain) - 190, stop=len(chain))\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mone_input_output_txes\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_input_output_txes' is not defined"
     ]
    }
   ],
   "source": [
    "%time one_input_output_txes = chain.find_hw_sw_coinjoins(start=len(chain) - 190, stop=len(chain))\n",
    "print(len(one_input_output_txes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cb7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(one_input_output_txes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7e9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842668\n"
     ]
    }
   ],
   "source": [
    "print(len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc243d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59min 45s, sys: 5min 53s, total: 1h 5min 39s\n",
      "Wall time: 22.9 s\n",
      "CPU times: user 1h 6min 40s, sys: 5.52 s, total: 1h 6min 45s\n",
      "Wall time: 29.8 s\n",
      "CPU times: user 1h 2min 36s, sys: 7.2 s, total: 1h 2min 43s\n",
      "Wall time: 28.7 s\n",
      "CPU times: user 1h 8min 33s, sys: 6.51 s, total: 1h 8min 39s\n",
      "Wall time: 25.7 s\n",
      "CPU times: user 54min 24s, sys: 4.41 s, total: 54min 28s\n",
      "Wall time: 19.9 s\n",
      "CPU times: user 53min 43s, sys: 5.37 s, total: 53min 49s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "strict = False\n",
    "%time w2_wp = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi2_events, whirlpool_events, strict)\n",
    "%time wp_w2 = chain.find_traverses_in_coinjoin_flows(0, len(chain), whirlpool_events, wasabi2_events, strict)\n",
    "%time w1_wp = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi_events, whirlpool_events, strict)\n",
    "%time wp_w1 = chain.find_traverses_in_coinjoin_flows(0, len(chain), whirlpool_events, wasabi_events, strict)\n",
    "%time w2_w1 = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi2_events, wasabi_events, strict)\n",
    "%time w1_w2 = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi_events, wasabi2_events, strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34229a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasabi2 -> Whirlpool Total length: 1194, case 1: 42, case 2: 1152\n",
      "Wasabi2 -> Whirlpool\n",
      "\tCASE 1 HOP: total: 2834782749 satoshis / 28.35 BTC\n",
      "\tCASE 2 HOPS: total: 51379000302 satoshis / 513.79 BTC\n",
      "Whirlpool -> Wasabi2 Total length: 3198, case 1: 1549, case 2: 1649\n",
      "Whirlpool -> Wasabi2\n",
      "\tCASE 1 HOP: total: 22507026905 satoshis / 225.07 BTC\n",
      "\tCASE 2 HOPS: total: 9515463713 satoshis / 95.15 BTC\n",
      "Wasabi -> Whirlpool Total length: 3183, case 1: 28, case 2: 3155\n",
      "Wasabi -> Whirlpool\n",
      "\tCASE 1 HOP: total: 265722659 satoshis / 2.66 BTC\n",
      "\tCASE 2 HOPS: total: 95272290375 satoshis / 952.72 BTC\n",
      "Whirlpool -> Wasabi Total length: 1660, case 1: 608, case 2: 1052\n",
      "Whirlpool -> Wasabi\n",
      "\tCASE 1 HOP: total: 8938035036 satoshis / 89.38 BTC\n",
      "\tCASE 2 HOPS: total: 10341313441 satoshis / 103.41 BTC\n",
      "Wasabi2 -> Wasabi Total length: 653, case 1: 394, case 2: 259\n",
      "Wasabi2 -> Wasabi\n",
      "\tCASE 1 HOP: total: 4108480212 satoshis / 41.08 BTC\n",
      "\tCASE 2 HOPS: total: 4218390033 satoshis / 42.18 BTC\n",
      "Wasabi -> Wasabi2 Total length: 7212, case 1: 2861, case 2: 4351\n",
      "Wasabi -> Wasabi2\n",
      "\tCASE 1 HOP: total: 56821126780 satoshis / 568.21 BTC\n",
      "\tCASE 2 HOPS: total: 75837318256 satoshis / 758.37 BTC\n",
      "11618\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Wasabi2 -> Whirlpool\", \"Whirlpool -> Wasabi2\", \"Wasabi -> Whirlpool\", \"Whirlpool -> Wasabi\", \"Wasabi2 -> Wasabi\", \"Wasabi -> Wasabi2\"]\n",
    "results = [w2_wp, wp_w2, w1_wp, wp_w1, w2_w1, w1_w2]\n",
    "\n",
    "def is_case_1(x):\n",
    "    return len(x[4]) == 0\n",
    "sums = 0\n",
    "\n",
    "for label, result in zip(labels, results):\n",
    "    print(label, f\"Total length: {len(result)}, case 1: {len(list(filter(is_case_1, result)))}, case 2: {len(list(filter(lambda x: not is_case_1(x) , result)))}\")\n",
    "    satoshis_case1 = sum(map(lambda x: x[3], filter(lambda x: is_case_1(x), result)))\n",
    "    satoshis_case2 = sum(map(lambda x: x[3], filter(lambda x: not is_case_1(x), result)))\n",
    "    print(label)\n",
    "    print(f\"\\tCASE 1 HOP: total: {satoshis_case1} satoshis / {round(satoshis_case1 / 100000000, 2)} BTC\")\n",
    "    print(f\"\\tCASE 2 HOPS: total: {satoshis_case2} satoshis / {round(satoshis_case2 / 100000000, 2)} BTC\")\n",
    "    \n",
    "    sums += len(list(filter(lambda x: not is_case_1(x), result)))\n",
    "    \n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463d134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = []\n",
    "\n",
    "for label, value in zip(labels, results):\n",
    "    for txid, in_cjs, out_cjs, value, pairs in value:\n",
    "        if len(pairs) != 0:\n",
    "            continue\n",
    "            \n",
    "        tx = chain.tx_with_hash(txid)\n",
    "            \n",
    "        r = {\n",
    "            \"txid\": str(tx.hash),\n",
    "            \"block_index\": tx.block_height,\n",
    "            \"broadcast_time\": tx.block_time.isoformat(),\n",
    "            \"num_inputs\": tx.input_count,\n",
    "            \"num_outputs\": tx.output_count,\n",
    "            \"in_cjs\": list(in_cjs),\n",
    "            \"out_cjs\": list(out_cjs),\n",
    "            \"flow_direction\": label,\n",
    "            \"hops\": 2,\n",
    "            \"sats_moved\": value,\n",
    "            \"hop_tx_cj_pairs\": {x: y for x, y in pairs}\n",
    "        }\n",
    "        \n",
    "        output_json.append(r)\n",
    "\n",
    "with open('/mnt/dumplings/one_hop_flows.json', 'w') as f:\n",
    "    json.dump(output_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e7a87cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c3ef7b51744ee16cf177b90c4ec7fc2df78a78272969754af02a6aaf626cfd04': 'adb90a7f1e3fb7dc2abf94727d783152ee24ace63c9c234ed69a8701b57a7c09'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_json[0][\"hop_tx_cj_pairs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad0dcb96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mix_flows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_results \u001b[38;5;241m=\u001b[39m {k: {} \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmix_flows\u001b[49m}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_map\u001b[39m(lst):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {x[\u001b[38;5;241m0\u001b[39m]: x[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m4\u001b[39m], lst)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mix_flows' is not defined"
     ]
    }
   ],
   "source": [
    "my_results = {k: {} for k in mix_flows}\n",
    "\n",
    "def to_map(lst):\n",
    "    return {x[0]: x[3] for x in filter(lambda x: x[4], lst)}\n",
    "\n",
    "keys = ['whirlpool_to_wasabi1', 'whirlpool_to_wasabi2', 'wasabi1_to_whirlpool', 'wasabi1_to_wasabi2', 'wasabi2_to_whirlpool', 'wasabi2_to_wasabi1']\n",
    "values = [wp_w1, wp_w2, w1_wp, w1_w2, w2_wp, w2_w1]\n",
    "\n",
    "for k, v in zip(keys, values):\n",
    "    mp = to_map(v)\n",
    "    my_results[k] = mp\n",
    "    mine, their, common = set(), set(), set()\n",
    "#     for tx, flow in mix_flows[k].items():\n",
    "#         if tx not in mp:\n",
    "#             their.add(tx)\n",
    "    \n",
    "#     for tx, flow in mp.items():\n",
    "#         if tx in mix_flows[k]:\n",
    "#             common.add(tx)\n",
    "    \n",
    "    \n",
    "#     print(len(mine), len(their), len(common))\n",
    "\n",
    "for k in my_results:\n",
    "    print(f\"{k}: standa: {len(my_results[k])}, petrs: {len(mix_flows[k])}\")\n",
    "\n",
    "for k, v in my_results['wasabi1_to_wasabi2'].items():\n",
    "    if k not in mix_flows['wasabi1_to_wasabi2']:\n",
    "        mine.add(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for txid in mine:\n",
    "    result.append(chain.tx_with_hash(txid))\n",
    "    \n",
    "sorted_results = sorted(result, key=lambda tx: tx.output_count)\n",
    "for i in range(10):\n",
    "    curr = sorted_results[i]\n",
    "    print(curr.hash, curr.input_count, curr.output_count, str(curr.hash) in mix_flows['wasabi1_to_wasabi2'])\n",
    "#     print(curr.hash)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61ca536a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'4ac7e9e2e58a220f866c265dd391137d83e35695ba052f8c5fc1307c0811d13a' in whirlpool_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3035004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broadcast_time': '2022-10-20 05:27:23.000', 'value': 9920232}\n",
      "Wasabi -> Wasabi2 8626\n",
      "Wasabi1 -> Whirlpool 27\n",
      "Wasabi2 -> Wasabi1 789\n",
      "Wasabi2 -> Whirlpool 42\n",
      "Whirlpool -> Wasabi1 630\n",
      "Whirlpool -> Wasabi2 1636\n",
      "4ac7e9e2e58a220f866c265dd391137d83e35695ba052f8c5fc1307c0811d13a\n",
      "7516d0146d6ca2bdb1dea717d25828537e73b949151338e295cd340b59e91a16\n",
      "e85266be2372837873e2141e0db363a047c485e6043934c5662aaed3b6211bfd\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/dumplings/mix_flows.json\", \"r\") as f:\n",
    "    mixed = json.load(f)\n",
    "    \n",
    "    \n",
    "print(mixed['Wasabi2 -> Wasabi1']['7e2083774fffa386464b5c35184337d6e5e80c8f5022248a63c31a9fe2584ea0'])\n",
    "for i in mixed:\n",
    "    print(i, len(mixed[i]))\n",
    "\n",
    "curr = set(map(lambda x: x[0], wp_w2))\n",
    "\n",
    "cnt = 0\n",
    "for i in mixed['Whirlpool -> Wasabi2']:\n",
    "    if i not in curr:\n",
    "        cnt += 1\n",
    "        if i not in wasabi2_events:\n",
    "            print(i)\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0586081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
